
# ğŸ“˜ SAÃ‰ 5.02 â€” Supervision rÃ©seau avec Docker et Ansible

## PrÃ©sentation du projet

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre de la **SAÃ‰ 5.02** du BUT RÃ©seaux & TÃ©lÃ©communications.
Il vise Ã  mettre en Å“uvre une **infrastructure rÃ©seau virtualisÃ©e**, reprÃ©sentative dâ€™un petit rÃ©seau dâ€™entreprise, tout en appliquant une dÃ©marche dâ€™**automatisation** et de **supervision**.

Lâ€™objectif nâ€™est pas uniquement de dÃ©ployer un rÃ©seau fonctionnel, mais Ã©galement de montrer comment celui-ci peut Ãªtre **supervisÃ©** afin de dÃ©tecter rapidement une anomalie ou une perte de connectivitÃ©.
Les informations de supervision sont centralisÃ©es dans un **dashboard**, permettant dâ€™avoir une vue globale et claire de lâ€™Ã©tat du rÃ©seau.

---

## DÃ©marche et objectifs

La dÃ©marche suivie dans ce projet est la suivante :

* Concevoir une architecture rÃ©seau simple et cohÃ©rente
* DÃ©ployer cette architecture Ã  lâ€™aide de Docker
* Automatiser la configuration rÃ©seau avec Ansible
* Structurer lâ€™automatisation en **rÃ´les Ansible** afin dâ€™Ã©viter les rÃ©pÃ©titions
* Mettre en place une **supervision centralisÃ©e**
* Visualiser lâ€™Ã©tat du rÃ©seau et les alertes dans un **dashboard de supervision**
* GÃ©nÃ©rer un rapport de supervision
* Tester le systÃ¨me par une simulation de panne

---

## Architecture du rÃ©seau

Lâ€™infrastructure repose sur **quatre conteneurs Docker** :

* **node-manager**
  Machine dâ€™administration exÃ©cutant Ansible et centralisant la supervision.

* **router-central**
  Routeur central assurant le routage entre les diffÃ©rents rÃ©seaux.

* **client1**
  HÃ´te du premier rÃ©seau local.

* **client2**
  HÃ´te du second rÃ©seau local.

Trois rÃ©seaux Docker sont utilisÃ©s :

* un rÃ©seau principal (**backbone**)
* deux rÃ©seaux locaux (**lan_client1** et **lan_client2**)

---

## Supervision et dashboard

La supervision du rÃ©seau permet de vÃ©rifier en continu la **disponibilitÃ© des Ã©quipements** et la **connectivitÃ© entre les hÃ´tes**.

Les rÃ©sultats de supervision (Ã©tat des hÃ´tes, succÃ¨s ou Ã©chec des tests de connectivitÃ©) sont :

* collectÃ©s automatiquement,
* centralisÃ©s sur le **node-manager**,
* affichÃ©s dans un **dashboard de supervision**.

Ce dashboard offre une vue synthÃ©tique de lâ€™Ã©tat du rÃ©seau :

* hÃ´tes joignables ou non,
* dÃ©tection rapide dâ€™une panne,
* visualisation claire des alertes.

Il permet ainsi de rÃ©agir rapidement en cas de problÃ¨me, comme dans un contexte rÃ©el de supervision rÃ©seau en entreprise.

---

## Organisation du projet

```
sae502-supervision/
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ ansible/
â”‚   â”œâ”€â”€ inventory/
â”‚   â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ roles/
â”‚   â””â”€â”€ reports/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_node_manager.sh
â””â”€â”€ README.md
```

---

## Automatisation, supervision et alertes

Le playbook de dÃ©ploiement configure automatiquement le rÃ©seau et prÃ©pare les hÃ´tes Ã  Ãªtre supervisÃ©s :

```bash
ansible-playbook ansible/playbooks/deploy.yml
```

Le playbook de supervision :

* teste la connectivitÃ© rÃ©seau,
* dÃ©tecte les hÃ´tes injoignables,
* met Ã  jour les informations affichÃ©es dans le dashboard,
* gÃ©nÃ¨re un rapport de supervision.

```bash
ansible-playbook ansible/playbooks/supervise.yml
```

---

## Simulation de panne

Une panne peut Ãªtre simulÃ©e en arrÃªtant un conteneur client :

```bash
docker stop client2
```

Cette panne est immÃ©diatement dÃ©tectÃ©e par la supervision et visible dans le **dashboard**, ce qui permet de valider le bon fonctionnement du systÃ¨me dâ€™alertes.

---

## Conclusion

Ce projet met en Å“uvre une infrastructure rÃ©seau automatisÃ©e et supervisÃ©e, proche dâ€™un cas rÃ©el.
Lâ€™utilisation dâ€™un **dashboard de supervision** permet dâ€™avoir une vision claire et centralisÃ©e de lâ€™Ã©tat du rÃ©seau, facilitant la dÃ©tection des incidents et lâ€™analyse du fonctionnement global.
